{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016049,
     "end_time": "2021-02-04T06:26:43.736727",
     "exception": false,
     "start_time": "2021-02-04T06:26:43.720678",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.01476,
     "end_time": "2021-02-04T06:26:43.767002",
     "exception": false,
     "start_time": "2021-02-04T06:26:43.752242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**This notebook is an exercise in the [Feature Engineering](https://www.kaggle.com/learn/feature-engineering) course.  You can reference the tutorial at [this link](https://www.kaggle.com/matleonard/feature-selection).**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015283,
     "end_time": "2021-02-04T06:26:43.797208",
     "exception": false,
     "start_time": "2021-02-04T06:26:43.781925",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction\n",
    "\n",
    "In this exercise you'll use some feature selection algorithms to improve your model. Some methods take a while to run, so you'll write functions and verify they work on small samples.\n",
    "\n",
    "To begin, run the code cell below to set up the exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:26:43.838290Z",
     "iopub.status.busy": "2021-02-04T06:26:43.837308Z",
     "iopub.status.idle": "2021-02-04T06:26:45.655091Z",
     "shell.execute_reply": "2021-02-04T06:26:45.654024Z"
    },
    "papermill": {
     "duration": 1.842732,
     "end_time": "2021-02-04T06:26:45.655405",
     "exception": false,
     "start_time": "2021-02-04T06:26:43.812673",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up code checking\n",
    "from learntools.core import binder\n",
    "binder.bind(globals())\n",
    "from learntools.feature_engineering.ex4 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.022201,
     "end_time": "2021-02-04T06:26:45.700858",
     "exception": false,
     "start_time": "2021-02-04T06:26:45.678657",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Then run the following cell. It takes a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:26:45.760146Z",
     "iopub.status.busy": "2021-02-04T06:26:45.759437Z",
     "iopub.status.idle": "2021-02-04T06:26:54.148447Z",
     "shell.execute_reply": "2021-02-04T06:26:54.147868Z"
    },
    "papermill": {
     "duration": 8.424635,
     "end_time": "2021-02-04T06:26:54.148597",
     "exception": false,
     "start_time": "2021-02-04T06:26:45.723962",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".datatable table.frame { margin-bottom: 0; }\n",
       ".datatable table.frame thead { border-bottom: none; }\n",
       ".datatable table.frame tr.coltypes td {  color: #FFFFFF;  line-height: 6px;  padding: 0 0.5em;}\n",
       ".datatable .bool    { background: #DDDD99; }\n",
       ".datatable .object  { background: #565656; }\n",
       ".datatable .int     { background: #5D9E5D; }\n",
       ".datatable .float   { background: #4040CC; }\n",
       ".datatable .str     { background: #CC4040; }\n",
       ".datatable .row_index {  background: var(--jp-border-color3);  border-right: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  font-size: 9px;}\n",
       ".datatable .frame tr.coltypes .row_index {  background: var(--jp-border-color0);}\n",
       ".datatable th:nth-child(2) { padding-left: 12px; }\n",
       ".datatable .hellipsis {  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .vellipsis {  background: var(--jp-layout-color0);  color: var(--jp-cell-editor-border-color);}\n",
       ".datatable .na {  color: var(--jp-cell-editor-border-color);  font-size: 80%;}\n",
       ".datatable .footer { font-size: 9px; }\n",
       ".datatable .frame_dimensions {  background: var(--jp-border-color3);  border-top: 1px solid var(--jp-border-color0);  color: var(--jp-ui-font-color3);  display: inline-block;  opacity: 0.6;  padding: 1px 10px 1px 5px;}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing, metrics\n",
    "import lightgbm as lgb\n",
    "\n",
    "import os\n",
    "\n",
    "clicks = pd.read_parquet('../input/feature-engineering-data/baseline_data.pqt')\n",
    "data_files = ['count_encodings.pqt',\n",
    "              'catboost_encodings.pqt',\n",
    "              'interactions.pqt',\n",
    "              'past_6hr_events.pqt',\n",
    "              'downloads.pqt',\n",
    "              'time_deltas.pqt',\n",
    "              'svd_encodings.pqt']\n",
    "data_root = '../input/feature-engineering-data'\n",
    "for file in data_files:\n",
    "    features = pd.read_parquet(os.path.join(data_root, file))\n",
    "    clicks = clicks.join(features)\n",
    "\n",
    "def get_data_splits(dataframe, valid_fraction=0.1):\n",
    "\n",
    "    dataframe = dataframe.sort_values('click_time')\n",
    "    valid_rows = int(len(dataframe) * valid_fraction)\n",
    "    train = dataframe[:-valid_rows * 2]\n",
    "    # valid size == test size, last two sections of the data\n",
    "    valid = dataframe[-valid_rows * 2:-valid_rows]\n",
    "    test = dataframe[-valid_rows:]\n",
    "    \n",
    "    return train, valid, test\n",
    "\n",
    "def train_model(train, valid, test=None, feature_cols=None):\n",
    "    if feature_cols is None:\n",
    "        feature_cols = train.columns.drop(['click_time', 'attributed_time',\n",
    "                                           'is_attributed'])\n",
    "    dtrain = lgb.Dataset(train[feature_cols], label=train['is_attributed'])\n",
    "    dvalid = lgb.Dataset(valid[feature_cols], label=valid['is_attributed'])\n",
    "    \n",
    "    param = {'num_leaves': 64, 'objective': 'binary', \n",
    "             'metric': 'auc', 'seed': 7}\n",
    "    num_round = 1000\n",
    "    print(\"Training model!\")\n",
    "    bst = lgb.train(param, dtrain, num_round, valid_sets=[dvalid], \n",
    "                    early_stopping_rounds=20, verbose_eval=False)\n",
    "    \n",
    "    valid_pred = bst.predict(valid[feature_cols])\n",
    "    valid_score = metrics.roc_auc_score(valid['is_attributed'], valid_pred)\n",
    "    print(f\"Validation AUC score: {valid_score}\")\n",
    "    \n",
    "    if test is not None: \n",
    "        test_pred = bst.predict(test[feature_cols])\n",
    "        test_score = metrics.roc_auc_score(test['is_attributed'], test_pred)\n",
    "        return bst, valid_score, test_score\n",
    "    else:\n",
    "        return bst, valid_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015543,
     "end_time": "2021-02-04T06:26:54.184101",
     "exception": false,
     "start_time": "2021-02-04T06:26:54.168558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Baseline Score\n",
    "\n",
    "Let's look at the baseline score for all the features we've made so far."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:26:54.221683Z",
     "iopub.status.busy": "2021-02-04T06:26:54.220993Z",
     "iopub.status.idle": "2021-02-04T06:29:06.301998Z",
     "shell.execute_reply": "2021-02-04T06:29:06.302563Z"
    },
    "papermill": {
     "duration": 132.102933,
     "end_time": "2021-02-04T06:29:06.302735",
     "exception": false,
     "start_time": "2021-02-04T06:26:54.199802",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "[LightGBM] [Info] Number of positive: 363974, number of negative: 1476475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.183100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 14325\n",
      "[LightGBM] [Info] Number of data points in the train set: 1840449, number of used features: 91\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.197764 -> initscore=-1.400330\n",
      "[LightGBM] [Info] Start training from score -1.400330\n",
      "Validation AUC score: 0.9659876667590743\n"
     ]
    }
   ],
   "source": [
    "train, valid, test = get_data_splits(clicks)\n",
    "_, baseline_score = train_model(train, valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.016848,
     "end_time": "2021-02-04T06:29:06.336859",
     "exception": false,
     "start_time": "2021-02-04T06:29:06.320011",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 1) Which data to use for feature selection?\n",
    "\n",
    "Since many feature selection methods require calculating statistics from the dataset, should you use all the data for feature selection?\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:29:06.378123Z",
     "iopub.status.busy": "2021-02-04T06:29:06.377261Z",
     "iopub.status.idle": "2021-02-04T06:29:06.390086Z",
     "shell.execute_reply": "2021-02-04T06:29:06.389394Z"
    },
    "papermill": {
     "duration": 0.035754,
     "end_time": "2021-02-04T06:29:06.390256",
     "exception": false,
     "start_time": "2021-02-04T06:29:06.354502",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"1_FeatureSelectionData\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> Including validation and test data within the feature selection is a source of leakage. You'll want to perform feature selection on the train set only, then use the results there to remove features from the validation and test sets."
      ],
      "text/plain": [
       "Solution: Including validation and test data within the feature selection is a source of leakage. You'll want to perform feature selection on the train set only, then use the results there to remove features from the validation and test sets."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.018298,
     "end_time": "2021-02-04T06:29:06.427719",
     "exception": false,
     "start_time": "2021-02-04T06:29:06.409421",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now we have 91 features we're using for predictions. With all these features, there is a good chance the model is overfitting the data. We might be able to reduce the overfitting by removing some features. Of course, the model's performance might decrease. But at least we'd be making the model smaller and faster without losing much performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017998,
     "end_time": "2021-02-04T06:29:06.465314",
     "exception": false,
     "start_time": "2021-02-04T06:29:06.447316",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 2) Univariate Feature Selection\n",
    "\n",
    "Below, use `SelectKBest` with the `f_classif` scoring function to choose 40 features from the 91 features in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:29:06.510576Z",
     "iopub.status.busy": "2021-02-04T06:29:06.509940Z",
     "iopub.status.idle": "2021-02-04T06:29:20.321385Z",
     "shell.execute_reply": "2021-02-04T06:29:20.320804Z"
    },
    "papermill": {
     "duration": 13.837616,
     "end_time": "2021-02-04T06:29:20.321549",
     "exception": false,
     "start_time": "2021-02-04T06:29:06.483933",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"2_UnivariateSelection\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc33\">Correct</span>"
      ],
      "text/plain": [
       "Correct"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "feature_cols = clicks.columns.drop(['click_time', 'attributed_time', 'is_attributed'])\n",
    "train, valid, test = get_data_splits(clicks)\n",
    "\n",
    "# Create the selector, keeping 40 features\n",
    "selector = SelectKBest(f_classif,k=40)\n",
    "\n",
    "# Use the selector to retrieve the best features\n",
    "X_new = selector.fit_transform(train[feature_cols],train['is_attributed'])\n",
    "\n",
    "# Get back the kept features as a DataFrame with dropped columns as all 0s\n",
    "selected_features = pd.DataFrame(selector.inverse_transform(X_new), index=train.index, columns=feature_cols)\n",
    "\n",
    "# Find the columns that were dropped\n",
    "dropped_columns = selected_features.columns[selected_features.var() == 0]\n",
    "\n",
    "# Check your answer\n",
    "q_2.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:29:20.366903Z",
     "iopub.status.busy": "2021-02-04T06:29:20.366067Z",
     "iopub.status.idle": "2021-02-04T06:29:20.371537Z",
     "shell.execute_reply": "2021-02-04T06:29:20.370887Z"
    },
    "papermill": {
     "duration": 0.029971,
     "end_time": "2021-02-04T06:29:20.371688",
     "exception": false,
     "start_time": "2021-02-04T06:29:20.341717",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment these lines if you need some guidance\n",
    "#q_2.hint()\n",
    "#q_2.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:29:20.441437Z",
     "iopub.status.busy": "2021-02-04T06:29:20.431462Z",
     "iopub.status.idle": "2021-02-04T06:30:40.299353Z",
     "shell.execute_reply": "2021-02-04T06:30:40.299872Z"
    },
    "papermill": {
     "duration": 79.908084,
     "end_time": "2021-02-04T06:30:40.300043",
     "exception": false,
     "start_time": "2021-02-04T06:29:20.391959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "[LightGBM] [Info] Number of positive: 363974, number of negative: 1476475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.884711 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 6258\n",
      "[LightGBM] [Info] Number of data points in the train set: 1840449, number of used features: 40\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.197764 -> initscore=-1.400330\n",
      "[LightGBM] [Info] Start training from score -1.400330\n",
      "Validation AUC score: 0.9625039444949599\n"
     ]
    }
   ],
   "source": [
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021101,
     "end_time": "2021-02-04T06:30:40.343707",
     "exception": false,
     "start_time": "2021-02-04T06:30:40.322606",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 3) The best value of K\n",
    "\n",
    "With this method we can choose the best K features, but we still have to choose K ourselves. How would you find the \"best\" value of K? That is, you want it to be small so you're keeping the best features, but not so small that it's degrading the model's performance.\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:30:40.394383Z",
     "iopub.status.busy": "2021-02-04T06:30:40.392998Z",
     "iopub.status.idle": "2021-02-04T06:30:40.402494Z",
     "shell.execute_reply": "2021-02-04T06:30:40.401591Z"
    },
    "papermill": {
     "duration": 0.037379,
     "end_time": "2021-02-04T06:30:40.402673",
     "exception": false,
     "start_time": "2021-02-04T06:30:40.365294",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"3_BestKValue\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> To find the best value of K, you can fit multiple models with increasing values of K, then choose the smallest K with validation score above some threshold or some other criteria. A good way to do this is loop over values of K and record the validation scores for each iteration."
      ],
      "text/plain": [
       "Solution: To find the best value of K, you can fit multiple models with increasing values of K, then choose the smallest K with validation score above some threshold or some other criteria. A good way to do this is loop over values of K and record the validation scores for each iteration."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034378,
     "end_time": "2021-02-04T06:30:40.471632",
     "exception": false,
     "start_time": "2021-02-04T06:30:40.437254",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 4) Use L1 regularization for feature selection\n",
    "\n",
    "Now try a more powerful approach using L1 regularization. Implement a function `select_features_l1` that returns a list of features to keep.\n",
    "\n",
    "Use a `LogisticRegression` classifier model with an L1 penalty to select the features. For the model, set:\n",
    "- the random state to 7,\n",
    "- the regularization parameter to 0.1,\n",
    "- and the solver to `'liblinear'`.\n",
    "\n",
    "Fit the model then use `SelectFromModel` to return a model with the selected features.\n",
    "\n",
    "The checking code will run your function on a sample from the dataset to provide more immediate feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:30:40.598029Z",
     "iopub.status.busy": "2021-02-04T06:30:40.596517Z",
     "iopub.status.idle": "2021-02-04T06:30:49.797049Z",
     "shell.execute_reply": "2021-02-04T06:30:49.796207Z"
    },
    "papermill": {
     "duration": 9.291024,
     "end_time": "2021-02-04T06:30:49.797254",
     "exception": false,
     "start_time": "2021-02-04T06:30:40.506230",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"outcomeType\": 1, \"valueTowardsCompletion\": 0.16666666666666666, \"interactionType\": 1, \"questionType\": 2, \"questionId\": \"4_L1Regularization\", \"learnToolsVersion\": \"0.3.4\", \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\"}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc33\">Correct</span>"
      ],
      "text/plain": [
       "Correct"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "def select_features_l1(X, y):\n",
    "    \"\"\"Return selected features using logistic regression with an L1 penalty.\"\"\"\n",
    "    lr = LogisticRegression(C=0.1,penalty=\"l1\",solver='liblinear',random_state=7).fit(X,y)\n",
    "    model = SelectFromModel(lr, prefit=True)\n",
    "    X_new=model.transform(X)\n",
    "    \n",
    "    selected_features=pd.DataFrame(model.inverse_transform(X_new),index=X.index,columns=X.columns)\n",
    "    keeped_cols= selected_features.columns[selected_features.var() !=0]\n",
    "\n",
    "    return keeped_cols\n",
    "\n",
    "# Check your answer\n",
    "q_4.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:30:49.876993Z",
     "iopub.status.busy": "2021-02-04T06:30:49.876324Z",
     "iopub.status.idle": "2021-02-04T06:30:49.879776Z",
     "shell.execute_reply": "2021-02-04T06:30:49.879091Z"
    },
    "papermill": {
     "duration": 0.046071,
     "end_time": "2021-02-04T06:30:49.879932",
     "exception": false,
     "start_time": "2021-02-04T06:30:49.833861",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment these if you're feeling stuck\n",
    "#q_4.hint()\n",
    "#q_4.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:30:49.985684Z",
     "iopub.status.busy": "2021-02-04T06:30:49.984966Z",
     "iopub.status.idle": "2021-02-04T06:31:58.496487Z",
     "shell.execute_reply": "2021-02-04T06:31:58.497190Z"
    },
    "papermill": {
     "duration": 68.592543,
     "end_time": "2021-02-04T06:31:58.497406",
     "exception": false,
     "start_time": "2021-02-04T06:30:49.904863",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model!\n",
      "[LightGBM] [Info] Number of positive: 363974, number of negative: 1476475\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.489847 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4507\n",
      "[LightGBM] [Info] Number of data points in the train set: 1840449, number of used features: 25\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.197764 -> initscore=-1.400330\n",
      "[LightGBM] [Info] Start training from score -1.400330\n",
      "Validation AUC score: 0.9656558241516175\n"
     ]
    }
   ],
   "source": [
    "n_samples = 10000\n",
    "X, y = train[feature_cols][:n_samples], train['is_attributed'][:n_samples]\n",
    "selected = select_features_l1(X, y)\n",
    "\n",
    "dropped_columns = feature_cols.drop(selected)\n",
    "_ = train_model(train.drop(dropped_columns, axis=1), \n",
    "                valid.drop(dropped_columns, axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038884,
     "end_time": "2021-02-04T06:31:58.575917",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.537033",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 5) Feature Selection with Trees\n",
    "\n",
    "Since we're using a tree-based model, using another tree-based model for feature selection might produce better results. What would you do different to select the features using a trees classifier?\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:31:58.661519Z",
     "iopub.status.busy": "2021-02-04T06:31:58.660371Z",
     "iopub.status.idle": "2021-02-04T06:31:58.670559Z",
     "shell.execute_reply": "2021-02-04T06:31:58.671257Z"
    },
    "papermill": {
     "duration": 0.055885,
     "end_time": "2021-02-04T06:31:58.671473",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.615588",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"5_FeatureSelectionTrees\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> You could use something like `RandomForestClassifier` or `ExtraTreesClassifier` to find feature importances. `SelectFromModel` can use the feature importances to find the best features."
      ],
      "text/plain": [
       "Solution: You could use something like `RandomForestClassifier` or `ExtraTreesClassifier` to find feature importances. `SelectFromModel` can use the feature importances to find the best features."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_5.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.027996,
     "end_time": "2021-02-04T06:31:58.727605",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.699609",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### 6) Top K features with L1 regularization\n",
    "\n",
    "Here you've set the regularization parameter `C=0.1` which led to some number of features being dropped. However, by setting `C` you aren't able to choose a certain number of features to keep. What would you do to keep the top K important features using L1 regularization?\n",
    "\n",
    "Run the following line after you've decided your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-02-04T06:31:58.788366Z",
     "iopub.status.busy": "2021-02-04T06:31:58.787436Z",
     "iopub.status.idle": "2021-02-04T06:31:58.793451Z",
     "shell.execute_reply": "2021-02-04T06:31:58.793914Z"
    },
    "papermill": {
     "duration": 0.039011,
     "end_time": "2021-02-04T06:31:58.794111",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.755100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "parent.postMessage({\"jupyterEvent\": \"custom.exercise_interaction\", \"data\": {\"interactionType\": 3, \"questionType\": 4, \"questionId\": \"6_L1SelectionTopK\", \"learnToolsVersion\": \"0.3.4\", \"valueTowardsCompletion\": 0.0, \"failureMessage\": \"\", \"exceptionClass\": \"\", \"trace\": \"\", \"outcomeType\": 4}}, \"*\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "<span style=\"color:#33cc99\">Solution:</span> To select a certain number of features with L1 regularization, you need to find the regularization parameter that leaves the desired number of features. To do this you can iterate over models with different regularization parameters from low to high and choose the one that leaves K features. Note that for the scikit-learn models C is the *inverse* of the regularization strength."
      ],
      "text/plain": [
       "Solution: To select a certain number of features with L1 regularization, you need to find the regularization parameter that leaves the desired number of features. To do this you can iterate over models with different regularization parameters from low to high and choose the one that leaves K features. Note that for the scikit-learn models C is the *inverse* of the regularization strength."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check your answer (Run this code cell to receive credit!)\n",
    "q_6.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028611,
     "end_time": "2021-02-04T06:31:58.851560",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.822949",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Congratulations on finishing this course! To keep learning, check out the rest of [our courses](https://www.kaggle.com/learn/overview). The machine learning explainability and deep learning courses are great next skills to learn!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.028156,
     "end_time": "2021-02-04T06:31:58.907818",
     "exception": false,
     "start_time": "2021-02-04T06:31:58.879662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "*Have questions or comments? Visit the [Learn Discussion forum](https://www.kaggle.com/learn-forum/161443) to chat with other Learners.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 323.213992,
   "end_time": "2021-02-04T06:31:59.748793",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-02-04T06:26:36.534801",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
